learning_rate: 5e-5
num_epochs: 1.0
per_device_train_batch_size: 1
gradient_accumulation_steps: 4
max_seq_length: 4096
max_steps: -1
finetuning_precision_mode: '4bit'
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.05
enable_gradient_checkpointing: true
attn_implementation: 'eager'
optimizer: 'paged_adamw_32bit'
warmup_ratio: 0.01
report_to: 'tensorboard'
save_steps: 10
logging_steps: 10
train_precision: 'bfloat16'